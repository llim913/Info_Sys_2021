{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"name":"lab-09_4_mnist_batchnorm.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6_Ukl7YavI89"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iw9q3M7PvN8I"},"source":["cd drive/My Drive/수업/Info_Sys_2021/PyTorch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdf2uQmkvHk9"},"source":["# Lab 10 MNIST and softmax\n","import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","\n","import matplotlib.pylab as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TwUy9FIvHlD"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# for reproducibility\n","torch.manual_seed(1)\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6oc5dCFvHlE"},"source":["# parameters\n","learning_rate = 0.01\n","training_epochs = 10\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOwbPbervHlE"},"source":["# MNIST dataset\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMhf4EZhvHlE"},"source":["# dataset loader\n","train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVth4rIOSWsS"},"source":["class MultiLayerModel_2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer1 = nn.Sequential(nn.Linear(28*28, 256), nn.ReLU()) \n","        self.layer2 = nn.Sequential(nn.Linear(256, 256), nn.ReLU())\n","        self.layer3 = nn.Sequential(nn.Linear(256, 64), nn.ReLU())\n","        self.layer4 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        out=self.layer1(x)\n","        out=self.layer2(out)\n","        out=self.layer3(out)\n","        out=self.layer4(out)\n","               \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9lOTcykTagF"},"source":["class MultiLayerModel_4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer1 = nn.Sequential(nn.Linear(28*28, 256), nn.BatchNorm1d(256), nn.ReLU()) \n","        self.layer2 = nn.Sequential(nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU())\n","        self.layer3 = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64),nn.ReLU()) \n","        self.layer4 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        out=self.layer1(x)\n","        out=self.layer2(out)\n","        out=self.layer3(out)\n","        out=self.layer4(out)\n","               \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ir7ylkjFTxLy"},"source":["nn_model = MultiLayerModel_2().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ri4nA0iQT6SK"},"source":["bn_model = MultiLayerModel_4().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWT6GHDg7M9W"},"source":["bn_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8exQdi86tLW"},"source":["pip install pytorch_model_summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewI1thNu6UU1"},"source":["import pytorch_model_summary\n","print(pytorch_model_summary.summary(bn_model, torch.zeros(1, 28*28).to(device), show_input=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1ZnMWgpvHlF"},"source":["# define cost/loss & optimizer\n","criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n","bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n","nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2k4_4ISrvHlG"},"source":["# Save Losses and Accuracies every epoch\n","# We are going to plot them later\n","train_losses = []\n","train_accs = []\n","\n","valid_losses = []\n","valid_accs = []\n","\n","train_total_batch = len(train_loader)\n","test_total_batch = len(test_loader)\n","for epoch in range(training_epochs):\n","    bn_model.train()  # set the model to train mode\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        bn_optimizer.zero_grad()\n","        bn_prediction = bn_model(X)\n","        bn_loss = criterion(bn_prediction, Y)\n","        bn_loss.backward()\n","        bn_optimizer.step()\n","\n","        nn_optimizer.zero_grad()\n","        nn_prediction = nn_model(X)\n","        nn_loss = criterion(nn_prediction, Y)\n","        nn_loss.backward()\n","        nn_optimizer.step()\n","\n","    with torch.no_grad():\n","        bn_model.eval()     # set the model to evaluation mode\n","\n","        # Test the model using train sets\n","        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n","        for i, (X, Y) in enumerate(train_loader):\n","            X = X.view(-1, 28 * 28).to(device)\n","            Y = Y.to(device)\n","\n","            bn_prediction = bn_model(X)\n","            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n","            bn_loss += criterion(bn_prediction, Y)\n","            bn_acc += bn_correct_prediction.float().mean()\n","\n","            nn_prediction = nn_model(X)\n","            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n","            nn_loss += criterion(nn_prediction, Y)\n","            nn_acc += nn_correct_prediction.float().mean()\n","\n","        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n","\n","        # Save train losses/acc\n","        train_losses.append([bn_loss, nn_loss])\n","        train_accs.append([bn_acc, nn_acc])\n","        print(\n","            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n","            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n","        # Test the model using test sets\n","        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n","        for i, (X, Y) in enumerate(test_loader):\n","            X = X.view(-1, 28 * 28).to(device)\n","            Y = Y.to(device)\n","\n","            bn_prediction = bn_model(X)\n","            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n","            bn_loss += criterion(bn_prediction, Y)\n","            bn_acc += bn_correct_prediction.float().mean()\n","\n","            nn_prediction = nn_model(X)\n","            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n","            nn_loss += criterion(nn_prediction, Y)\n","            nn_acc += nn_correct_prediction.float().mean()\n","\n","        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n","\n","        # Save valid losses/acc\n","        valid_losses.append([bn_loss, nn_loss])\n","        valid_accs.append([bn_acc, nn_acc])\n","        print(\n","            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n","                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n","        print()\n","\n","print('Learning finished')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2V0IQHwvHlH"},"source":["def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n","    bn = [i[0] for i in loss_list]\n","    nn = [i[1] for i in loss_list]\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.plot(bn, label='With BN')\n","    plt.plot(nn, label='Without BN')\n","    if ylim:\n","        plt.ylim(ylim)\n","\n","    if title:\n","        plt.title(title)\n","    plt.legend()\n","    plt.grid('on')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11h4wzn-vHlI"},"source":["plot_compare(train_losses, title='Training Loss at Epoch')\n","plot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\n","plot_compare(valid_losses, title='Validation Loss at Epoch')\n","plot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"],"execution_count":null,"outputs":[]}]}