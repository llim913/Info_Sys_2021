{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lab-11_1_autoencoder_64.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"code","metadata":{"Collapsed":"false","id":"LKrL0-f8Nev2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"NFXKHTT9NgRv"},"source":["cd drive/My Drive/class20211/Info_Sys_2021/class_note_Info_sys/PyTorch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL_UVDqvCJM5"},"source":["from IPython.display import Image\n","Image('AE_1.jpg', width=600, height=200)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USQjTygUCJXG"},"source":["from IPython.display import Image\n","Image('CAE_1.png', width=600, height=200)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"cuhNxv51NL_K"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchsummary import summary\n","from torch.utils.data import DataLoader\n","\n","#from pushover import notify\n","#from utils import makegif\n","from random import randint\n","\n","from IPython.display import Image\n","from IPython.core.display import Image, display\n","\n","#%load_ext autoreload\n","#%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"EBcQsmu9NL_R"},"source":["batch_size = 32\n","num_epochs= 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"6Trl-9_MNL_Q"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"ddT_0e1_klUZ"},"source":["trans = transforms.Compose([\n","    transforms.Resize((64,64)),\n","    transforms.ToTensor()\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUJLKGw9MqmE"},"source":["train_data = torchvision.datasets.ImageFolder(root='train', transform=trans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_PI4xbPMo93"},"source":["train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEpiHlwcNxaG"},"source":["len(train_data.imgs),len(train_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"_DJU2_-CNL_S"},"source":["# Fixed input for debugging\n","fixed_x, _ = next(iter(train_loader))\n","print(fixed_x.shape)\n","save_image(fixed_x, 'real_image.png')\n","\n","Image('real_image.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"IWX6quCNNL_S"},"source":["class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"88r3vAlANL_T"},"source":["class UnFlatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), 32, 14, 14)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"hb_WB_CANL_T"},"source":["class Autoencoder(nn.Module):\n","    def __init__(self, image_channels=3, z_dim=10):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(image_channels, 16, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            Flatten(),\n","        \n","            nn.Linear(6272, 256),\n","            nn.Linear(256, z_dim)\n","            )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim, 256),\n","            nn.Linear(256, 6272),\n","        \n","            UnFlatten(),\n","        \n","            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2),\n","            nn.ReLU(),\n","        \n","            nn.ConvTranspose2d(16, image_channels, kernel_size=4, stride=2),\n","            nn.Sigmoid()\n","        )\n","        \n","\n","\n","    def forward(self, x):\n","        out =self.encoder(x)\n","        out = self.decoder(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"tpFCYlgnNL_U"},"source":["image_channels = fixed_x.size(1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"0yRzg7x7qI7_"},"source":["model = Autoencoder(image_channels=image_channels).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRRzzY_YRM3k"},"source":["pip install pytorch_model_summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uWE-TkARM5Z"},"source":["import pytorch_model_summary\n","print(pytorch_model_summary.summary(model, torch.zeros(1, 3, 64, 64).to(device), show_input=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qcb6KDZ1Z0Ot"},"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n","\n","# set to training mode\n","model.train()\n","\n","train_loss_avg = []\n","\n","print('Training ...')\n","for epoch in range(num_epochs):\n","    train_loss_avg.append(0)\n","    num_batches = 0\n","    \n","    for image_batch, _ in train_loader:\n","        \n","        image_batch = image_batch.to(device)\n","        \n","        # autoencoder reconstruction\n","        image_batch_recon = model(image_batch)\n","        \n","        # reconstruction error\n","        loss = F.mse_loss(image_batch_recon, image_batch)\n","        \n","        # backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        # one step of the optmizer (using the gradients from backpropagation)\n","        optimizer.step()\n","        \n","        train_loss_avg[-1] += loss.item()\n","        num_batches += 1\n","        \n","    train_loss_avg[-1] /= num_batches\n","    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ux2D8WIxcUbM"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","plt.plot(train_loss_avg)\n","plt.xlabel('Epochs')\n","plt.ylabel('Reconstruction error')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnxXBlaYfMmz"},"source":["def compare(x):\n","    recon_x= model(x)\n","    return torch.cat([x, recon_x])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"F94x7AZsNL_W"},"source":["fixed_x = train_data[randint(1, 100)][0].unsqueeze(0)\n","compare_x = compare(fixed_x.to(device))\n","\n","save_image(compare_x.data.cpu(), 'sample_image.png')\n","display(Image('sample_image.png', width=300, unconfined=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"M3g-MlKPNL_W"},"source":[""],"execution_count":null,"outputs":[]}]}