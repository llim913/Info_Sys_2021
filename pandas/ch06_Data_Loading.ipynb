{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6.  Data Loading, Storage, and File Formats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input and output typically falls into a few main categories: reading text files and other\n",
    "more efficient on-disk formats, loading data from databases, and interacting with net‐\n",
    "work sources like web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1  Reading and Writing Data in Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas features a number of functions for reading tabular data as a DataFrame\n",
    "object. \n",
    "* Table 6-1 summarizes some of them, though **read_csv** and **read_table** are\n",
    "likely the ones you’ll use the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pic/pic_6_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pic/pic_6_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll give an overview of the mechanics of these functions, which are meant to convert\n",
    "text data into a DataFrame. \n",
    "\n",
    "\n",
    "The optional arguments for these functions may fall into\n",
    "a few categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Indexing*  \n",
    "Can treat one or more columns as the returned DataFrame, and whether to get\n",
    "column names from the file, the user, or not at all.\n",
    "\n",
    "\n",
    "* *Type inference and data conversion*  \n",
    "This includes the user-defined value conversions and custom list of missing value\n",
    "markers.\n",
    "\n",
    "\n",
    "* *Datetime parsing*  \n",
    "Includes combining capability, including combining date and time information\n",
    "spread over multiple columns into a single column in the result.\n",
    "\n",
    "\n",
    "* *Iterating*  \n",
    "Support for iterating over chunks of very large files.\n",
    "\n",
    "\n",
    "* *Unclean data issues*  \n",
    "Skipping rows or a footer, comments, or other minor things like numeric data\n",
    "with thousands separated by commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how messy data in the real world can be, some of the data loading functions (especially **read_csv**) have grown very complex in their options over time. \n",
    "\n",
    "It’s\n",
    "normal to feel overwhelmed by the number of different parameters (**read_csv** has\n",
    "over 50 as of this writing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of these functions, like **pandas.read_csv**, perform *type inference*, because the\n",
    "column data types are not part of the data format.   \n",
    "* That means you don’t necessarily\n",
    "have to specify which columns are numeric, integer, boolean, or string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Handling dates and other custom types can require extra effort. \n",
    "\n",
    "Let’s start with a\n",
    "small comma-separated (CSV) text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples/ex1.csv 화일을 엑셀로 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is comma-delimited, we can use **read_csv** to read it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('examples/ex1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have used **read_table** and specified the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file will not always have a header row. Consider this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples/ex2.csv 화일을 엑셀로 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read this file, you have a couple of options. You can allow pandas to assign default\n",
    "column names, or you can specify names yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted the message column to be the index of the returned DataFrame.\n",
    "You can either indicate you want the column at index 4 or named 'message' using\n",
    "the index_col argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['a', 'b', 'c', 'd', 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', names=names, index_col='message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', names=names, index_col=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the event that you want to form a hierarchical index from multiple columns, pass a\n",
    "list of column numbers or names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples/csv_mindex.csv 화일을 엑셀로 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = pd.read_csv('examples/csv_mindex.csv',\n",
    "                     index_col=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, a table might not have a fixed delimiter, using whitespace or some\n",
    "other pattern to separate fields. Consider a text file that looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex3.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you could do some munging by hand, the fields here are separated by a vari‐\n",
    "able amount of whitespace. In these cases, you can pass a regular expression as a\n",
    "delimiter for **read_table**. This can be expressed by the regular expression **\\s+**, so we\n",
    "have then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whitespace is any character or series of characters that represent horizontal or vertical space in printing.\n",
    "\n",
    "<pre>\n",
    "‘ ‘ – Space\n",
    "‘\\t’ – Horizontal tab\n",
    "‘\\n’ – Newline\n",
    "‘\\v’ – Vertical tab\n",
    "‘\\f’ – Feed\n",
    "‘\\r’ – Carriage return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_table('examples/ex3.txt', sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구분자가 길이가 정해지지 않은 공백인 경우에는 \\s+라는 정규식(regular expression) 문자열을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex3.txt', sep='\\s+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there was one fewer column name than the number of data rows,\n",
    "**read_table** infers that the first column should be the DataFrame’s index in this special case.\n",
    "\n",
    "The parser functions have many additional arguments to help you handle the wide\n",
    "variety of exception file formats that occur (see a partial listing in Table 6-2). For\n",
    "example, you can skip the first, third, and fourth rows of a file with **skiprows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples/ex4.csv 화일을 엑셀로 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex4.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values is an important and frequently nuanced part of the file parsing process. Missing data is usually either not present (empty string) or marked by\n",
    "some sentinel value. By default, pandas uses a set of commonly occurring sentinels,\n",
    "such as **NA** and **NULL**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer programming, a sentinel value (also referred to as a flag value, trip value, rogue value, signal value, or dummy data) is a special value in the context of an algorithm which uses its presence as a condition of termination, typically in a loop or recursive algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex5.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('examples/ex5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **na_values** option can take either a list or set of strings to consider missing\n",
    "values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different NA sentinels can be specified for each column in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinels = {'message': ['foo', 'NA'], 'something': ['two']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex5.csv', na_values=sentinels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 6-2 lists some frequently used options in **pandas.read_csv** and **pan\n",
    "das.read_table**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pic/pic_6_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pic/pic_6_6.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing very large files or figuring out the right set of arguments to correctly process a large file, you may only want to read in a small piece of a file or iterate\n",
    "through smaller chunks of the file.\n",
    "\n",
    "Before we look at a large file, we make the pandas display settings more compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('examples/ex6.csv')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to only read a small number of rows (avoiding reading the entire file),\n",
    "specify that with **nrows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex6.csv', nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a file in pieces, specify a **chunksize** as a number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextParser object returned by **read_csv** allows you to iterate over the parts of\n",
    "the file according to the **chunksize**.   \n",
    "For example, we can iterate over ex6.csv, aggre‐\n",
    "gating the value counts in the 'key' column like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('examples/ex6.csv', chunksize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = pd.Series([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot.add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for piece in chunker:\n",
    "    print(piece)\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "    print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot.sort_values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = tot.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be exported to a delimited format.   \n",
    "Let’s consider one of the CSV files\n",
    "read before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex5.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('examples/ex5.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DataFrame’s **to_csv** method, we can write the data out to a comma-separated\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('examples/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/out.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other delimiters can be used, of course (writing to **sys.stdout** so it prints the text\n",
    "result to the console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "data.to_csv(sys.stdout, sep='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values appear as empty strings in the output.  \n",
    "You might want to denote them\n",
    "by some other sentinel value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(sys.stdout, na_rep='NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no other options specified, both the row and column labels are written. Both of\n",
    "these can be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(sys.stdout, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write only a subset of the columns, and in an order of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series also has a **to_csv** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('1/1/2000', periods=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(np.arange(7), index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.to_csv('examples/tseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/tseries.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delimited Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s possible to load most forms of tabular data from disk using functions like **pandas.read_table**.   \n",
    "In some cases, however, some manual processing may be necessary.  \n",
    "It’s not uncommon to receive a file with one or more malformed lines that trip up\n",
    "**read_table**.   \n",
    "To illustrate the basic tools, consider a small CSV file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex7.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any file with a single-character delimiter, you can use Python’s built-in **csv** module.   \n",
    "To use it, pass any open file or file-like object to csv.reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('examples/ex7.csv')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the reader like a file yields tuples of values with any quote characters removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, it’s up to you to do the wrangling necessary to put the data in the form\n",
    "that you need it.   \n",
    "Let’s take this step by step.   \n",
    "First, we read the file into a list of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples/ex7.csv') as f:\n",
    "    lines = list(csv.reader(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we split the lines into the header line and the data lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, values = lines[0], lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a dictionary of data columns using a dictionary comprehension\n",
    "and the expression zip(*values), which transposes rows to columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {h: v for h, v in zip(header, zip(values))} #zip(values)는 ( )안에 values 하나 밖에 없으므로 ([1,2,3],없음) 이 된다.\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {h: v for h, v in zip(header, values)}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[ 1, 2, 3],[ 4, 5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(*A)) #zip(*a) is equal to zip(a[0], a[1], a[2], ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(A[0],A[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(A[0],A[1]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([1,2,3],[4,5,6],[7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* JSON (short for JavaScript Object Notation) has become one of the standard formats\n",
    "for sending data by HTTP request between web browsers and other applications. \n",
    "* It is\n",
    "a much more free-form data format than a tabular text form like CSV. \n",
    "\n",
    "Here is an\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38,\n",
    "               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* JSON is very nearly valid Python code with the exception of its null value null and\n",
    "some other nuances (such as disallowing trailing commas at the end of lists). \n",
    "* The\n",
    "basic types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. \n",
    "* All\n",
    "of the keys in an object must be strings. \n",
    "* There are several Python libraries for reading and writing JSON data. \n",
    "* I’ll use json here, as it is built into the Python standard\n",
    "library. \n",
    "* To convert a JSON string to Python form, use **json.loads**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json.dumps, on the other hand, converts a Python object back to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(asjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you convert a JSON object or list of objects to a DataFrame or some other data\n",
    "structure for analysis will be up to you.   \n",
    "Conveniently, you can pass a list of dicts\n",
    "(which were previously JSON objects) to the DataFrame constructor and select a subset of the data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings = pd.DataFrame(result['siblings'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas.read_json** can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/example.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default options for **pandas.read_json** assume that each object in the JSON array\n",
    "is a row in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('examples/example.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to export data from pandas to JSON, one way is to use the **to_json** methods on Series and DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML and HTML: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python has many libraries for reading and writing data in the ubiquitous HTML and\n",
    "XML formats. \n",
    "* Examples include lxml, Beautiful Soup, and html5lib. \n",
    "* While lxml is\n",
    "comparatively much faster in general, the other libraries can better handle malformed\n",
    "HTML or XML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas has a built-in function, **read_html**, which uses libraries like lxml and Beautiful Soup to automatically parse tables out of HTML files as DataFrame objects. \n",
    "\n",
    "To\n",
    "show how this works, I downloaded an HTML file (used in the pandas documenta‐\n",
    "tion) from the United States FDIC government agency showing bank failures.  \n",
    "First,\n",
    "you must install some additional libraries used by **read_html**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Courier New; font-size: 1.15em;\">\n",
    "conda install lxml\n",
    "\n",
    "<p style=\"font-family: Courier New; font-size: 1.15em;\">\n",
    "pip install beautifulsoup4 html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **pandas.read_html** function has a number of options, but by default it searches\n",
    "for and attempts to parse all tabular data contained within < table > tags. \n",
    "    \n",
    "The result is\n",
    "a list of DataFrame objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples/fdic_failed_bank_list.html 을 클릭하여 구글 chrome으로 열기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모장으로 examples/fdic_failed_bank_list.html 연 후, 찾기 < table 을 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('examples/fdic_failed_bank_list.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing XML with lxml.objectify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2  Binary Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the easiest ways to store data (also known as serialization) efficiently in binary\n",
    "format is using Python’s built-in **pickle** serialization. \n",
    "* pandas objects all have a\n",
    "**to_pickle** method that writes the data to disk in pickle format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(open('examples/ex1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('examples/ex1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_pickle('examples/frame_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('examples/frame_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pic/pic_0_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">pickle is only recommended as a short-term storage format. The\n",
    "problem is that it is hard to guarantee that the format will be stable\n",
    "over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF5 Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Microsoft Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas also supports reading tabular data stored in Excel 2003 (and higher) files\n",
    "using either the **ExcelFile** class or **pandas.read_excel** function. \n",
    "* Internally these\n",
    "tools use the add-on packages xlrd and openpyxl to read XLS and XLSX files, respectively. \n",
    "* You may need to install these manually with pip or conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_excel('examples/ex2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many websites have public APIs providing data feeds via JSON or some other format.\n",
    "* There are a number of ways to access these APIs from Python; one easy-to-use\n",
    "method that I recommend is the requests package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Response object’s **json** method will return a dictionary containing JSON parsed\n",
    "into native Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in data is a dictionary containing all of the data found on a GitHub\n",
    "issue page (except for the comments).   \n",
    "We can pass data directly to DataFrame and\n",
    "extract fields of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = pd.DataFrame(data, columns=['number', 'title',\n",
    "                                     'labels', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In a business setting, most data may not be stored in text or Excel files. \n",
    "* SQL-based\n",
    "relational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use,\n",
    "and many alternative databases have become quite popular. \n",
    "* The choice of database is\n",
    "usually dependent on the performance, data integrity, and scalability needs of an\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading data from SQL into a DataFrame is fairly straightforward, and pandas has\n",
    "some functions to simplify the process. As an example, \n",
    "* I’ll create a SQLite database\n",
    "using Python’s built-in **sqlite3** driver:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(생략) sqlite 관련 화일은 sqlite_new.zip 에 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
